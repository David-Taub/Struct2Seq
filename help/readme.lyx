#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 1
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\headheight 0cm
\headsep 0cm
\footskip 0cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation 0bp
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset Formula $\gamma$
\end_inset

 - discount rate
\end_layout

\begin_layout Standard
The rollout:
\end_layout

\begin_layout Standard
states- 
\begin_inset Formula $s_{0},s_{1},...s_{n}\in\mathcal{S}$
\end_inset


\end_layout

\begin_layout Standard
rewards- 
\begin_inset Formula $r_{0,}r_{1},...,r_{n}\in\mathbb{R}$
\end_inset


\end_layout

\begin_layout Standard
actions- 
\begin_inset Formula $a_{0},a_{1},...,a_{n}\in\mathcal{A}$
\end_inset


\end_layout

\begin_layout Standard
discounted reward:
\end_layout

\begin_layout Standard
\begin_inset Formula $R_{i}=\sum_{j=i}^{n}\gamma^{j}r_{j}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $V^{(t)}(s_{i})$
\end_inset

 is the value output of our DNN, which is trained in steps t=0,1,2...
\end_layout

\begin_layout Standard
\begin_inset Formula $v_{i}^{(t)}=V^{(t)}(s_{i})\in\mathbb{R}$
\end_inset

 
\end_layout

\begin_layout Standard
During training we hope that 
\begin_inset Formula $v_{i}^{(t)}\underset{t\rightarrow\infty}{\rightarrow}R_{i}$
\end_inset

, therefore 
\begin_inset Formula $R_{i}$
\end_inset

 is called 
\series bold
target value
\end_layout

\begin_layout Standard
Policy:
\end_layout

\begin_layout Standard
\begin_inset Formula $P(s_{i})$
\end_inset

 is a policy output of our DNN, which is also trained in steps t=0,1,2...
\end_layout

\begin_layout Standard
\begin_inset Formula $P(s_{i})\in[0,1]^{|\mathcal{A}|}$
\end_inset

 this is a vector that sums to 1 and is the certainty of each possible action
 (softmax).
\end_layout

\begin_layout Standard
\begin_inset Formula $\pi(s_{i})=argmax_{j}(P_{j}(s_{i}))$
\end_inset


\end_layout

\begin_layout Standard
the certainty for 
\begin_inset Formula $a_{i}$
\end_inset

 that was taken at state 
\begin_inset Formula $s_{i}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula $p_{i}=P_{a_{i}}^{(t)}(s_{i})$
\end_inset


\end_layout

\begin_layout Standard

\series bold
policy gradient
\series default
, calculated as a discounted 
\series bold
advantage
\series default
:
\end_layout

\begin_layout Standard
\begin_inset Formula $\phi_{i}^{(t)}=\sum_{j=i}^{n}\gamma^{j}\left(r_{j}+\gamma v_{j+1}^{(t)}-v_{j}^{(t)}\right)$
\end_inset


\end_layout

\begin_layout Standard
this tells how much should the 
\begin_inset Formula $v_{i}$
\end_inset

 be improved knowing the new rewards that were observed.
\end_layout

\begin_layout Standard
Loss:
\end_layout

\begin_layout Standard
\begin_inset Formula $L_{v}=\sum\left(R_{i}-v_{i}\right)^{2}$
\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Formula $L_{p}=-\sum_{i}\left(log(p_{i})\cdot\phi_{i}\right)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $L_{e}=-\sum_{j}P_{j}^{(t)}(s_{i})\cdot log(P_{j}^{(t)}(s_{i}))$
\end_inset


\end_layout

\begin_layout Standard
Alternative loss:
\end_layout

\begin_layout Standard
\begin_inset Formula $L=-\sum_{j}log(p_{i})\cdot R_{j}$
\end_inset


\end_layout

\end_body
\end_document
